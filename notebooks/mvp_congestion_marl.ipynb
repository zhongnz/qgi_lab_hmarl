{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quantitative Green Infrastructure (QGI) for Urban Flood Mitigation \u2014 MVP Congestion MARL Notebook\n",
        "\n",
        "**Supervision:** Prof. Hang Ma, Supervised by QGI Lab (HMARL Team)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "This notebook narrates a minimum viable product (MVP) for a multi-agent reinforcement learning (MARL) approach to urban flood mitigation using quantitative green infrastructure (QGI). It is a self-contained, lightweight scaffold designed to run in Colab with standard packages and provide clear next steps for full implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Statement\n",
        "Urban flooding is exacerbated by climate change and rapid urbanization. We seek a scalable decision-making system that allocates green infrastructure actions across multiple agents (e.g., neighborhood catchments) to reduce peak congestion (inflows) and minimize flooding impacts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Research Questions\n",
        "1. How can MARL coordinate distributed QGI decisions to reduce flood risk?\n",
        "2. How do forecasting accuracy and uncertainty affect control performance?\n",
        "3. What is the best tradeoff between mitigation performance and deployment cost?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Methodology\n",
        "We combine (1) a synthetic congestion signal generator, (2) a forecasting module, and (3) a multi-agent controller. The MVP focuses on wiring these components with simple placeholders that can be expanded into a full pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Architecture Diagram (text)\n",
        "```\n",
        "[Rain/Runoff Signals] -> [Forecasting Module] -> [Multi-Agent Controller]\n",
        "        |                          |                     |\n",
        "        v                          v                     v\n",
        "   [Synthetic Data]           [Predictions]        [QGI Actions]\n",
        "        |                                                |\n",
        "        v                                                v\n",
        "           [Environment / Hydrology Surrogate (Gymnasium)]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MDP Formulation\n",
        "- **States (s):** recent runoff/congestion history + forecast summary + local storage.\n",
        "- **Actions (a):** per-agent QGI actions (e.g., retention, infiltration, diversion).\n",
        "- **Transition (P):** hydrology surrogate driven by rainfall + actions.\n",
        "- **Reward (r):** negative congestion/flooding + penalties for action costs.\n",
        "- **Episode:** fixed horizon (e.g., 24\u201372 timesteps)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forecasting Module\n",
        "We start with a trivial baseline (persistence or AR-like) and a minimal PyTorch LSTM stub that can be expanded into a full model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Minimal packages (standard Colab installs)\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Synthetic congestion signal generator\n",
        "\n",
        "def generate_congestion_series(T=96, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    t = np.arange(T)\n",
        "    # Simple periodic pattern + noise\n",
        "    signal = 1.0 + 0.5 * np.sin(2 * np.pi * t / 24) + 0.1 * rng.normal(size=T)\n",
        "    return np.clip(signal, 0.0, None)\n",
        "\n",
        "series = generate_congestion_series()\n",
        "series[:10]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Forecasting baseline: persistence\n",
        "\n",
        "def forecast_persistence(history, horizon=6):\n",
        "    if len(history) == 0:\n",
        "        return np.zeros(horizon)\n",
        "    return np.full(horizon, history[-1])\n",
        "\n",
        "forecast_persistence(series[:10], horizon=4)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Optional PyTorch LSTM placeholder (kept minimal)\n",
        "try:\n",
        "    import torch\n",
        "    import torch.nn as nn\n",
        "\n",
        "    class TinyLSTM(nn.Module):\n",
        "        def __init__(self, input_size=1, hidden_size=8):\n",
        "            super().__init__()\n",
        "            self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "            self.head = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            out, _ = self.lstm(x)\n",
        "            return self.head(out[:, -1])\n",
        "\n",
        "    model = TinyLSTM()\n",
        "except Exception as exc:\n",
        "    print(\"PyTorch not available in this environment:\", exc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RL Algorithm / Coordination\n",
        "The MVP uses a minimal MAPPO-style loop skeleton without heavy dependencies. This is a pseudocode-like structure designed for expansion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Minimal Gymnasium-style environment placeholder\n",
        "\n",
        "class FloodEnv:\n",
        "    def __init__(self, n_agents=3, horizon=24):\n",
        "        self.n_agents = n_agents\n",
        "        self.horizon = horizon\n",
        "        self.t = 0\n",
        "        self.state = None\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        self.t = 0\n",
        "        self.state = np.zeros(self.n_agents, dtype=float)\n",
        "        return self.state\n",
        "\n",
        "    def step(self, actions):\n",
        "        # Placeholder transition: next state is damped by actions\n",
        "        actions = np.asarray(actions)\n",
        "        congestion = np.maximum(self.state + 0.2 - 0.1 * actions, 0.0)\n",
        "        reward = -congestion.sum() - 0.05 * (actions ** 2).sum()\n",
        "        self.state = congestion\n",
        "        self.t += 1\n",
        "        terminated = self.t >= self.horizon\n",
        "        info = {\"congestion\": congestion}\n",
        "        return self.state, reward, terminated, False, info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Simple step loop for sanity check\n",
        "\n",
        "env = FloodEnv(n_agents=3, horizon=5)\n",
        "state = env.reset()\n",
        "for _ in range(5):\n",
        "    actions = np.zeros(env.n_agents)\n",
        "    state, reward, terminated, truncated, info = env.step(actions)\n",
        "    print(state, reward, terminated)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# MAPPO-like training loop skeleton (pseudocode)\n",
        "\n",
        "n_agents = 3\n",
        "n_episodes = 2\n",
        "\n",
        "env = FloodEnv(n_agents=n_agents, horizon=10)\n",
        "\n",
        "for ep in range(n_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    ep_return = 0.0\n",
        "\n",
        "    while not done:\n",
        "        # Placeholder policy: zero actions\n",
        "        actions = np.zeros(n_agents)\n",
        "        next_obs, reward, terminated, truncated, info = env.step(actions)\n",
        "        done = terminated or truncated\n",
        "        ep_return += reward\n",
        "        obs = next_obs\n",
        "\n",
        "    print(f\"Episode {ep} return: {ep_return:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baselines\n",
        "- Rule-based (static) QGI allocation.\n",
        "- Centralized single-agent RL (no coordination).\n",
        "- Forecasting: persistence vs. naive moving average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Expected Contributions\n",
        "- A minimal, reproducible MARL pipeline for QGI control.\n",
        "- Insights into coordination vs. centralized control.\n",
        "- Benchmarks for forecasting + control coupling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experimental Plan / Data\n",
        "- Start with synthetic signals and simplified hydrology surrogate.\n",
        "- Introduce real rainfall/runoff datasets when available.\n",
        "- Compare MARL vs. baselines using cumulative congestion and cost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Timeline\n",
        "1. **Week 1\u20132:** MVP scaffolding + synthetic data.\n",
        "2. **Week 3\u20134:** Forecasting module integration.\n",
        "3. **Week 5\u20136:** MAPPO training + baseline comparison.\n",
        "4. **Week 7+:** Real data integration + refinement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "This notebook provides a lightweight, executable template for a QGI congestion MARL MVP. It outlines the problem, methodology, and a minimal code scaffold to be expanded into a full experimental pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n",
        "- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*.\n",
        "- Yu, C. et al. (2021). *The Surprising Effectiveness of PPO in Cooperative MARL*.\n",
        "- Urban hydrology and green infrastructure survey literature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "- [ ] Replace synthetic data with real rainfall/runoff series.\n",
        "- [ ] Implement proper hydrology surrogate or link to SWMM.\n",
        "- [ ] Add MAPPO policy/value networks and rollout storage.\n",
        "- [ ] Evaluate coordination vs. centralized baselines.\n",
        "- [ ] Add uncertainty-aware forecasting."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
