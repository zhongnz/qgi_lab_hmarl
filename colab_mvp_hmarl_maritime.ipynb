{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hierarchical Multi-Agent Reinforcement Learning for Congestion-Aware Vessel Scheduling\n",
        "\n",
        "**Supervised by Prof. Aboussalah**  \\n",
        "**Spring 2026 Independent Study**\n",
        "\n",
        "This Colab notebook is an MVP (minimum viable prototype) that sketches a working pipeline for hierarchical MARL with congestion forecasting and port coordination. The goal is to be **descriptive**, **traceable**, and **executable** with simplified components so we can iterate quickly before scaling to a full simulator and MAPPO training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of contents\n",
        "1. Project framing and objectives\n",
        "2. Architecture and data flow\n",
        "3. Configuration\n",
        "4. Toy simulator (MVP environment)\n",
        "5. Forecasting module (mock)\n",
        "6. Agent decision stubs (hierarchical control)\n",
        "7. Metrics and evaluation hooks\n",
        "8. MVP execution, visualization, and baselines\n",
        "9. Next steps for the full project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Project framing and objectives\n",
        "We model a maritime network with **heterogeneous agents**:\n",
        "- **Fleet coordinator** (strategic decisions, 12\u201324h cadence)\n",
        "- **Vessel agents** (operational speed/arrival control, 1\u20134h cadence)\n",
        "- **Port agents** (dock allocation and service scheduling, 2\u20136h cadence)\n",
        "\n",
        "**MVP objectives**:\n",
        "- Validate the **information flow** between forecasting, coordinator, vessel, and port layers.\n",
        "- Provide a **minimal environment** to test reward signals and coordination logic.\n",
        "- Establish **hooks for metrics** that will be used in the full study.\n",
        "\n",
        "We simulate a small system (1 coordinator, 8 vessels, 5 ports) and use a toy environment to demonstrate the **data flow** and **learning loops**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Architecture and data flow\n",
        "```\n",
        "Fleet Coordinator  \u2192 (Strategic Directives) \u2192  Vessel Agents  \u2192 (Arrival Requests) \u2192  Port Agents\n",
        "Port Agents        \u2192 (Dock Availability)   \u2192  Vessel Agents\n",
        "```\n",
        "**Forecasting usage**:\n",
        "- Medium-term forecasts (3\u20137 days) inform the **fleet coordinator**.\n",
        "- Short-term forecasts (6\u201324 hours) inform **vessels and ports**.\n",
        "\n",
        "This MVP will keep policies simple but will expose the exact data each agent sees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 MDP summary (MVP view)\n",
        "We keep the MDP definition lightweight here and focus on the observation/action channels that will later map to Gymnasium spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mdp_summary = pd.DataFrame(\n",
        "    [\n",
        "        {\n",
        "            \"agent\": \"Fleet Coordinator\",\n",
        "            \"observations\": \"Medium-term forecasts, vessel states, emissions\",\n",
        "            \"actions\": \"Destination port, departure window, emission budget\",\n",
        "            \"reward\": \"- (voyage cost + emission penalty)\",\n",
        "        },\n",
        "        {\n",
        "            \"agent\": \"Vessel\",\n",
        "            \"observations\": \"Short-term forecasts, position/speed, directives\",\n",
        "            \"actions\": \"Speed adjustment, arrival request\",\n",
        "            \"reward\": \"- (fuel + delay + emissions)\",\n",
        "        },\n",
        "        {\n",
        "            \"agent\": \"Port\",\n",
        "            \"observations\": \"Queue, dock occupancy, arrival requests\",\n",
        "            \"actions\": \"Dock allocation, service rate\",\n",
        "            \"reward\": \"- (queue wait + dock idle)\",\n",
        "        },\n",
        "    ]\n",
        ")\n",
        "mdp_summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration\n",
        "We keep dependencies minimal for the MVP. In a full implementation, this section will include MARL frameworks (e.g., RLlib, MARLlib) and forecasting libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from copy import deepcopy\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "SEED = 42\n",
        "rng = np.random.default_rng(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Core experiment parameters\n",
        "We centralize key parameters so the team can adjust horizons, weights, and counts in one place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    \"num_ports\": 5,\n",
        "    \"num_vessels\": 8,\n",
        "    \"medium_horizon_days\": 5,\n",
        "    \"short_horizon_hours\": 12,\n",
        "    \"fuel_weight\": 1.0,\n",
        "    \"delay_weight\": 1.5,\n",
        "    \"emission_weight\": 0.7,\n",
        "}\n",
        "CONFIG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Toy simulator (MVP environment)\n",
        "We implement a **discrete-event toy simulator** that captures the key signals: queue lengths, dock occupancy, and travel times. This is intentionally small and interpretable so we can debug coordination and forecast usage early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PortState:\n",
        "    queue: int\n",
        "    docks: int\n",
        "    occupied: int\n",
        "\n",
        "@dataclass\n",
        "class VesselState:\n",
        "    location: int\n",
        "    speed: float\n",
        "    fuel: float\n",
        "\n",
        "def initialize_ports(num_ports: int, docks_per_port: int = 3) -> List[PortState]:\n",
        "    return [PortState(queue=rng.integers(0, 5), docks=docks_per_port, occupied=0)\n",
        "            for _ in range(num_ports)]\n",
        "\n",
        "def initialize_vessels(num_vessels: int, num_ports: int) -> List[VesselState]:\n",
        "    return [VesselState(location=rng.integers(0, num_ports), speed=12.0, fuel=100.0)\n",
        "            for _ in range(num_vessels)]\n",
        "\n",
        "NUM_PORTS = CONFIG[\"num_ports\"]\n",
        "NUM_VESSELS = CONFIG[\"num_vessels\"]\n",
        "ports = initialize_ports(NUM_PORTS)\n",
        "vessels = initialize_vessels(NUM_VESSELS, NUM_PORTS)\n",
        "ports, vessels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Simple environment step (placeholder)\n",
        "We define a minimal **step** function to evolve queues and dock occupancy. This provides a concrete hook for plugging in rewards and metrics later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def step_ports(ports: List[PortState], service_rates: List[int]) -> None:\n",
        "    for port, rate in zip(ports, service_rates):\n",
        "        served = min(port.queue, rate)\n",
        "        port.queue = max(port.queue - served, 0)\n",
        "        port.occupied = min(port.docks, port.occupied + served)\n",
        "\n",
        "def observe_port_metrics(ports: List[PortState]) -> Dict[str, float]:\n",
        "    avg_queue = float(np.mean([p.queue for p in ports]))\n",
        "    dock_util = float(np.mean([p.occupied / p.docks for p in ports]))\n",
        "    return {\"avg_queue\": avg_queue, \"dock_utilization\": dock_util}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Forecasting module (mock)\n",
        "The **medium-term forecaster** provides a 3\u20137 day congestion estimate for each port. The **short-term forecaster** outputs 6\u201324 hour predictions. Here we mock them with noisy trends to validate the data flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def medium_term_forecast(num_ports: int, horizon_days: int = 5) -> np.ndarray:\n",
        "    base = rng.uniform(2, 8, size=(num_ports, 1))\n",
        "    trend = np.linspace(0, 1.5, horizon_days)[None, :]\n",
        "    noise = rng.normal(0, 0.3, size=(num_ports, horizon_days))\n",
        "    return np.clip(base + trend + noise, 0, None)\n",
        "\n",
        "def short_term_forecast(num_ports: int, horizon_hours: int = 12) -> np.ndarray:\n",
        "    base = rng.uniform(1, 6, size=(num_ports, 1))\n",
        "    noise = rng.normal(0, 0.5, size=(num_ports, horizon_hours))\n",
        "    return np.clip(base + noise, 0, None)\n",
        "\n",
        "medium_forecast = medium_term_forecast(NUM_PORTS, horizon_days=CONFIG[\"medium_horizon_days\"])\n",
        "short_forecast = short_term_forecast(NUM_PORTS, horizon_hours=CONFIG[\"short_horizon_hours\"])\n",
        "medium_forecast.shape, short_forecast.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Agent decision stubs (hierarchical control)\n",
        "We create placeholder policies that **consume forecasts** and **emit actions**. These are not learned yet; they simply show the flow of information and will be replaced with MAPPO policies later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fleet_coordinator_policy(medium_forecast: np.ndarray) -> Dict:\n",
        "    # pick the least congested port (lowest mean forecast)\n",
        "    port_scores = medium_forecast.mean(axis=1)\n",
        "    dest_port = int(np.argmin(port_scores))\n",
        "    return {\n",
        "        \"dest_port\": dest_port,\n",
        "        \"departure_window_hours\": 12,\n",
        "        \"emission_budget\": 50.0\n",
        "    }\n",
        "\n",
        "def vessel_policy(vessel: VesselState, short_forecast: np.ndarray, directive: Dict) -> Dict:\n",
        "    # reduce speed if short-term congestion is high\n",
        "    dest_port = directive[\"dest_port\"]\n",
        "    congestion = float(short_forecast[dest_port].mean())\n",
        "    speed = 10.0 if congestion > 4.0 else 14.0\n",
        "    return {\n",
        "        \"target_speed\": speed,\n",
        "        \"request_arrival_slot\": True,\n",
        "    }\n",
        "\n",
        "def port_policy(port_state: PortState, incoming_requests: int, short_forecast_row: np.ndarray) -> Dict:\n",
        "    # prioritize clearing queue when forecast predicts near-term congestion\n",
        "    pressure = float(short_forecast_row.mean())\n",
        "    service_rate = min(port_state.docks, port_state.occupied + 1)\n",
        "    if pressure > 4.0:\n",
        "        service_rate = port_state.docks  # open all docks\n",
        "    return {\n",
        "        \"service_rate\": service_rate,\n",
        "        \"accept_requests\": incoming_requests,\n",
        "    }\n",
        "\n",
        "directive = fleet_coordinator_policy(medium_forecast)\n",
        "v_actions = [vessel_policy(v, short_forecast, directive) for v in vessels]\n",
        "incoming = sum(1 for a in v_actions if a[\"request_arrival_slot\"])\n",
        "p_actions = [port_policy(p, incoming, short_forecast[i]) for i, p in enumerate(ports)]\n",
        "service_rates = [a[\"service_rate\"] for a in p_actions]\n",
        "step_ports(ports, service_rates)\n",
        "metrics = observe_port_metrics(ports)\n",
        "directive, v_actions[0], p_actions[0], metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Metrics and evaluation hooks\n",
        "We attach simple metrics now, so that later experiments can compare independent vs reactive vs predictive policies using the same pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_vessel_metrics(vessels: List[VesselState]) -> Dict[str, float]:\n",
        "    avg_speed = float(np.mean([v.speed for v in vessels]))\n",
        "    avg_fuel = float(np.mean([v.fuel for v in vessels]))\n",
        "    return {\"avg_speed\": avg_speed, \"avg_fuel\": avg_fuel}\n",
        "\n",
        "port_metrics = observe_port_metrics(ports)\n",
        "vessel_metrics = compute_vessel_metrics(vessels)\n",
        "{\"port_metrics\": port_metrics, \"vessel_metrics\": vessel_metrics}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Reward placeholder\n",
        "We expose a simple cost function that later becomes the per-agent reward in PPO/MAPPO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_vessel_cost(vessel: VesselState, delay_hours: float) -> float:\n",
        "    fuel_cost = CONFIG[\"fuel_weight\"] * vessel.speed\n",
        "    delay_cost = CONFIG[\"delay_weight\"] * delay_hours\n",
        "    emission_cost = CONFIG[\"emission_weight\"] * vessel.speed\n",
        "    return fuel_cost + delay_cost + emission_cost\n",
        "\n",
        "# Example placeholder usage\n",
        "compute_vessel_cost(vessels[0], delay_hours=2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. MVP execution and visualization\n",
        "We run a short rollout to show the end-to-end data flow. This is **not** training yet; it is a deterministic loop that uses the policy stubs and logs metrics so we can debug the system before integrating MAPPO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rollout(steps: int = 10) -> pd.DataFrame:\n",
        "    log = []\n",
        "    for t in range(steps):\n",
        "        medium = medium_term_forecast(NUM_PORTS, horizon_days=CONFIG[\"medium_horizon_days\"])\n",
        "        short = short_term_forecast(NUM_PORTS, horizon_hours=CONFIG[\"short_horizon_hours\"])\n",
        "        directive = fleet_coordinator_policy(medium)\n",
        "        v_actions = [vessel_policy(v, short, directive) for v in vessels]\n",
        "        incoming = sum(1 for a in v_actions if a[\"request_arrival_slot\"])\n",
        "        p_actions = [port_policy(p, incoming, short[i]) for i, p in enumerate(ports)]\n",
        "        service_rates = [a[\"service_rate\"] for a in p_actions]\n",
        "        step_ports(ports, service_rates)\n",
        "        port_metrics = observe_port_metrics(ports)\n",
        "        vessel_metrics = compute_vessel_metrics(vessels)\n",
        "        log.append({\n",
        "            \"t\": t,\n",
        "            **port_metrics,\n",
        "            **vessel_metrics,\n",
        "        })\n",
        "    return pd.DataFrame(log)\n",
        "\n",
        "rollout_df = rollout(steps=12)\n",
        "rollout_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Visualization hook\n",
        "We plot a few metrics so the team can quickly validate that the loop is producing sensible outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "ax[0].plot(rollout_df[\"t\"], rollout_df[\"avg_queue\"], label=\"Avg Queue\")\n",
        "ax[0].set_title(\"Average Queue Length\")\n",
        "ax[0].set_xlabel(\"Step\")\n",
        "ax[0].set_ylabel(\"Queue\")\n",
        "\n",
        "ax[1].plot(rollout_df[\"t\"], rollout_df[\"dock_utilization\"], label=\"Dock Util\")\n",
        "ax[1].set_title(\"Dock Utilization\")\n",
        "ax[1].set_xlabel(\"Step\")\n",
        "ax[1].set_ylabel(\"Utilization\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Baseline comparison (forecast vs reactive)\n",
        "We add a tiny comparison loop to highlight how forecasts change decisions. This stays lightweight but sets the structure for ablation studies later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def reactive_vessel_policy(vessel: VesselState, directive: Dict) -> Dict:\n",
        "    # ignores forecasts; always uses a nominal speed\n",
        "    return {\n",
        "        \"target_speed\": 12.0,\n",
        "        \"request_arrival_slot\": True,\n",
        "    }\n",
        "\n",
        "def run_policy_comparison(steps: int = 8) -> pd.DataFrame:\n",
        "    records = []\n",
        "    for t in range(steps):\n",
        "        medium = medium_term_forecast(NUM_PORTS, horizon_days=CONFIG[\"medium_horizon_days\"])\n",
        "        short = short_term_forecast(NUM_PORTS, horizon_hours=CONFIG[\"short_horizon_hours\"])\n",
        "        directive = fleet_coordinator_policy(medium)\n",
        "\n",
        "        # Forecast-aware policy (run on a cloned state)\n",
        "        ports_forecast = deepcopy(ports)\n",
        "        v_actions = [vessel_policy(v, short, directive) for v in vessels]\n",
        "        incoming = sum(1 for a in v_actions if a[\"request_arrival_slot\"])\n",
        "        p_actions = [port_policy(p, incoming, short[i]) for i, p in enumerate(ports_forecast)]\n",
        "        service_rates = [a[\"service_rate\"] for a in p_actions]\n",
        "        step_ports(ports_forecast, service_rates)\n",
        "        forecast_metrics = observe_port_metrics(ports_forecast)\n",
        "\n",
        "        # Reactive policy (run on a cloned state)\n",
        "        ports_reactive = deepcopy(ports)\n",
        "        v_actions_reactive = [reactive_vessel_policy(v, directive) for v in vessels]\n",
        "        incoming_reactive = sum(1 for a in v_actions_reactive if a[\"request_arrival_slot\"])\n",
        "        p_actions_reactive = [port_policy(p, incoming_reactive, short[i]) for i, p in enumerate(ports_reactive)]\n",
        "        service_rates_reactive = [a[\"service_rate\"] for a in p_actions_reactive]\n",
        "        step_ports(ports_reactive, service_rates_reactive)\n",
        "        reactive_metrics = observe_port_metrics(ports_reactive)\n",
        "\n",
        "        records.append({\n",
        "            \"t\": t,\n",
        "            \"forecast_avg_queue\": forecast_metrics[\"avg_queue\"],\n",
        "            \"reactive_avg_queue\": reactive_metrics[\"avg_queue\"],\n",
        "        })\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "comparison_df = run_policy_comparison()\n",
        "comparison_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.3 MVP-to-Research checklist\n",
        "This checklist makes explicit what is **already covered** and what remains for the research-grade implementation.\n",
        "\n",
        "**Covered in MVP**:\n",
        "- Hierarchical information flow (forecast \u2192 coordinator \u2192 vessel/port).\n",
        "- Minimal environment state and queue dynamics.\n",
        "- Metric logging hooks.\n",
        "\n",
        "**Next additions**:\n",
        "- Full Gymnasium env with multi-agent observation/action spaces.\n",
        "- MAPPO training loop with centralized critic.\n",
        "- Forecast model training/validation and forecast-sharing ablations.\n",
        "- Economic impact analysis (costs, price reliability metrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Next steps for the full project\n",
        "1. **Replace toy forecasts** with real models (RNNs, econometric baselines).\n",
        "2. **Implement Gymnasium environment** with step() and reset() for MARL training.\n",
        "3. **Plug in MAPPO** with a centralized critic for coordination.\n",
        "4. **Run ablations** comparing: independent vs reactive vs predictive vs oracle.\n",
        "5. **Evaluate** using cost, delay, emissions, and coordination metrics.\n",
        "\n",
        "This MVP ensures the data flow and hierarchy are correct before scaling to a production-grade simulator and full MARL training."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "mvp_hmarl_maritime.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
