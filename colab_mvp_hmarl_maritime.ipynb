{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hierarchical Multi-Agent Reinforcement Learning for Congestion-Aware Vessel Scheduling\n",
        "\n",
        "**Supervised by Prof. Aboussalah**  \\n",
        "**Spring 2026 Independent Study**\n",
        "\n",
        "This Colab notebook is an MVP (minimum viable prototype) that sketches a working pipeline for hierarchical MARL with congestion forecasting and port coordination. The goal is to be **descriptive**, **traceable**, and **executable** with simplified components so we can iterate quickly before scaling to a full simulator and MAPPO training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of contents\n",
        "1. Project framing and objectives\n",
        "2. Architecture and data flow\n",
        "3. Configuration\n",
        "4. Toy simulator (MVP environment)\n",
        "5. Forecasting module (mock)\n",
        "6. Agent decision stubs (hierarchical control)\n",
        "7. Metrics and evaluation hooks\n",
        "8. Next steps for the full project\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Project framing and objectives\n",
        "We model a maritime network with **heterogeneous agents**:\n",
        "- **Fleet coordinator** (strategic decisions, 12\u201324h cadence)\n",
        "- **Vessel agents** (operational speed/arrival control, 1\u20134h cadence)\n",
        "- **Port agents** (dock allocation and service scheduling, 2\u20136h cadence)\n",
        "\n",
        "**MVP objectives**:\n",
        "- Validate the **information flow** between forecasting, coordinator, vessel, and port layers.\n",
        "- Provide a **minimal environment** to test reward signals and coordination logic.\n",
        "- Establish **hooks for metrics** that will be used in the full study.\n",
        "\n",
        "We simulate a small system (1 coordinator, 8 vessels, 5 ports) and use a toy environment to demonstrate the **data flow** and **learning loops**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Architecture and data flow\n",
        "```\n",
        "Fleet Coordinator  \u2192 (Strategic Directives) \u2192  Vessel Agents  \u2192 (Arrival Requests) \u2192  Port Agents\n",
        "Port Agents        \u2192 (Dock Availability)   \u2192  Vessel Agents\n",
        "```\n",
        "**Forecasting usage**:\n",
        "- Medium-term forecasts (3\u20137 days) inform the **fleet coordinator**.\n",
        "- Short-term forecasts (6\u201324 hours) inform **vessels and ports**.\n",
        "\n",
        "This MVP will keep policies simple but will expose the exact data each agent sees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Configuration\n",
        "We keep dependencies minimal for the MVP. In a full implementation, this section will include MARL frameworks (e.g., RLlib, MARLlib) and forecasting libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "SEED = 42\n",
        "rng = np.random.default_rng(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Toy simulator (MVP environment)\n",
        "We implement a **discrete-event toy simulator** that captures the key signals: queue lengths, dock occupancy, and travel times. This is intentionally small and interpretable so we can debug coordination and forecast usage early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class PortState:\n",
        "    queue: int\n",
        "    docks: int\n",
        "    occupied: int\n",
        "\n",
        "@dataclass\n",
        "class VesselState:\n",
        "    location: int\n",
        "    speed: float\n",
        "    fuel: float\n",
        "\n",
        "def initialize_ports(num_ports: int, docks_per_port: int = 3) -> List[PortState]:\n",
        "    return [PortState(queue=rng.integers(0, 5), docks=docks_per_port, occupied=0)\n",
        "            for _ in range(num_ports)]\n",
        "\n",
        "def initialize_vessels(num_vessels: int, num_ports: int) -> List[VesselState]:\n",
        "    return [VesselState(location=rng.integers(0, num_ports), speed=12.0, fuel=100.0)\n",
        "            for _ in range(num_vessels)]\n",
        "\n",
        "NUM_PORTS = 5\n",
        "NUM_VESSELS = 8\n",
        "ports = initialize_ports(NUM_PORTS)\n",
        "vessels = initialize_vessels(NUM_VESSELS, NUM_PORTS)\n",
        "ports, vessels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Simple environment step (placeholder)\n",
        "We define a minimal **step** function to evolve queues and dock occupancy. This provides a concrete hook for plugging in rewards and metrics later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def step_ports(ports: List[PortState], service_rates: List[int]) -> None:\n",
        "    for port, rate in zip(ports, service_rates):\n",
        "        served = min(port.queue, rate)\n",
        "        port.queue = max(port.queue - served, 0)\n",
        "        port.occupied = min(port.docks, port.occupied + served)\n",
        "\n",
        "def observe_port_metrics(ports: List[PortState]) -> Dict[str, float]:\n",
        "    avg_queue = float(np.mean([p.queue for p in ports]))\n",
        "    dock_util = float(np.mean([p.occupied / p.docks for p in ports]))\n",
        "    return {\"avg_queue\": avg_queue, \"dock_utilization\": dock_util}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Forecasting module (mock)\n",
        "The **medium-term forecaster** provides a 3\u20137 day congestion estimate for each port. The **short-term forecaster** outputs 6\u201324 hour predictions. Here we mock them with noisy trends to validate the data flow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def medium_term_forecast(num_ports: int, horizon_days: int = 5) -> np.ndarray:\n",
        "    base = rng.uniform(2, 8, size=(num_ports, 1))\n",
        "    trend = np.linspace(0, 1.5, horizon_days)[None, :]\n",
        "    noise = rng.normal(0, 0.3, size=(num_ports, horizon_days))\n",
        "    return np.clip(base + trend + noise, 0, None)\n",
        "\n",
        "def short_term_forecast(num_ports: int, horizon_hours: int = 12) -> np.ndarray:\n",
        "    base = rng.uniform(1, 6, size=(num_ports, 1))\n",
        "    noise = rng.normal(0, 0.5, size=(num_ports, horizon_hours))\n",
        "    return np.clip(base + noise, 0, None)\n",
        "\n",
        "medium_forecast = medium_term_forecast(NUM_PORTS, horizon_days=5)\n",
        "short_forecast = short_term_forecast(NUM_PORTS, horizon_hours=12)\n",
        "medium_forecast.shape, short_forecast.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Agent decision stubs (hierarchical control)\n",
        "We create placeholder policies that **consume forecasts** and **emit actions**. These are not learned yet; they simply show the flow of information and will be replaced with MAPPO policies later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fleet_coordinator_policy(medium_forecast: np.ndarray) -> Dict:\n",
        "    # pick the least congested port (lowest mean forecast)\n",
        "    port_scores = medium_forecast.mean(axis=1)\n",
        "    dest_port = int(np.argmin(port_scores))\n",
        "    return {\n",
        "        \"dest_port\": dest_port,\n",
        "        \"departure_window_hours\": 12,\n",
        "        \"emission_budget\": 50.0\n",
        "    }\n",
        "\n",
        "def vessel_policy(vessel: VesselState, short_forecast: np.ndarray, directive: Dict) -> Dict:\n",
        "    # reduce speed if short-term congestion is high\n",
        "    dest_port = directive[\"dest_port\"]\n",
        "    congestion = float(short_forecast[dest_port].mean())\n",
        "    speed = 10.0 if congestion > 4.0 else 14.0\n",
        "    return {\n",
        "        \"target_speed\": speed,\n",
        "        \"request_arrival_slot\": True,\n",
        "    }\n",
        "\n",
        "def port_policy(port_state: PortState, incoming_requests: int, short_forecast_row: np.ndarray) -> Dict:\n",
        "    # prioritize clearing queue when forecast predicts near-term congestion\n",
        "    pressure = float(short_forecast_row.mean())\n",
        "    service_rate = min(port_state.docks, port_state.occupied + 1)\n",
        "    if pressure > 4.0:\n",
        "        service_rate = port_state.docks  # open all docks\n",
        "    return {\n",
        "        \"service_rate\": service_rate,\n",
        "        \"accept_requests\": incoming_requests,\n",
        "    }\n",
        "\n",
        "directive = fleet_coordinator_policy(medium_forecast)\n",
        "v_actions = [vessel_policy(v, short_forecast, directive) for v in vessels]\n",
        "incoming = sum(1 for a in v_actions if a[\"request_arrival_slot\"])\n",
        "p_actions = [port_policy(p, incoming, short_forecast[i]) for i, p in enumerate(ports)]\n",
        "service_rates = [a[\"service_rate\"] for a in p_actions]\n",
        "step_ports(ports, service_rates)\n",
        "metrics = observe_port_metrics(ports)\n",
        "directive, v_actions[0], p_actions[0], metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Metrics and evaluation hooks\n",
        "We attach simple metrics now, so that later experiments can compare independent vs reactive vs predictive policies using the same pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_vessel_metrics(vessels: List[VesselState]) -> Dict[str, float]:\n",
        "    avg_speed = float(np.mean([v.speed for v in vessels]))\n",
        "    avg_fuel = float(np.mean([v.fuel for v in vessels]))\n",
        "    return {\"avg_speed\": avg_speed, \"avg_fuel\": avg_fuel}\n",
        "\n",
        "port_metrics = observe_port_metrics(ports)\n",
        "vessel_metrics = compute_vessel_metrics(vessels)\n",
        "{\"port_metrics\": port_metrics, \"vessel_metrics\": vessel_metrics}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Next steps for the full project\n",
        "1. **Replace toy forecasts** with real models (RNNs, econometric baselines).\n",
        "2. **Implement Gymnasium environment** with step() and reset() for MARL training.\n",
        "3. **Plug in MAPPO** with a centralized critic for coordination.\n",
        "4. **Run ablations** comparing: independent vs reactive vs predictive vs oracle.\n",
        "5. **Evaluate** using cost, delay, emissions, and coordination metrics.\n",
        "\n",
        "This MVP ensures the data flow and hierarchy are correct before scaling to a production-grade simulator and full MARL training."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "mvp_hmarl_maritime.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
